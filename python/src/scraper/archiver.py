"""Forum archiver - orchestrates extraction and downloading

Two-phase archive process:
1. Collect all post URLs from author pages
2. For each post: extract details, download media, save content

Features:
- Incremental archiving (skip completed posts)
- Resume capability (post-level and file-level)
- Progress tracking with .progress and .complete markers
- Rate limiting to avoid anti-scraping measures
"""

import asyncio
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List
import re

from .extractor import PostExtractor
from .downloader import MediaDownloader
from .utils import (
    sanitize_filename,
    should_archive,
    mark_complete,
    get_archive_progress,
    save_archive_progress
)
from ..utils.logger import setup_logger


class ForumArchiver:
    """è®ºå›å½’æ¡£å™¨ï¼ˆåè°ƒ Extractor + Downloaderï¼‰"""

    def __init__(self, config: dict):
        """Initialize archiver

        Args:
            config: Configuration dictionary from config.yaml
        """
        self.config = config

        # Extract forum URL (base URL)
        section_url = config['forum']['section_url']
        # Parse base URL from section URL (e.g., https://t66y.com/...)
        match = re.match(r'(https?://[^/]+)', section_url)
        self.base_url = match.group(1) if match else section_url

        self.archive_dir = Path(config['storage']['archive_path'])

        # Setup logging
        project_root = Path(__file__).parent.parent.parent.parent
        log_dir = project_root / 'logs'
        log_dir.mkdir(exist_ok=True)

        self.logger = setup_logger('archiver', log_dir)

        # Initialize sub-components
        self.extractor = PostExtractor(self.base_url, log_dir)
        self.downloader = MediaDownloader(
            max_concurrent=config.get('advanced', {}).get('max_concurrent', 5),
            retry_count=config.get('advanced', {}).get('download_retry', 3),
            timeout=config.get('advanced', {}).get('download_timeout', 30),
            log_dir=log_dir
        )

        # Rate limiting delay
        self.rate_limit_delay = config.get('advanced', {}).get('rate_limit_delay', 0.5)

        # Download settings
        self.download_images = config.get('storage', {}).get('download', {}).get('images', True)
        self.download_videos = config.get('storage', {}).get('download', {}).get('videos', True)

    async def archive_author(
        self,
        author_name: str,
        author_url: str,
        max_pages: Optional[int] = None
    ) -> Dict:
        """å½’æ¡£ä½œè€…çš„æ‰€æœ‰å¸–å­

        Args:
            author_name: Author name
            author_url: Author's post list URL
            max_pages: Maximum pages to scrape (None = all)

        Returns:
            Statistics dict with keys: total, new, skipped, failed
        """
        self.logger.info(f"=" * 60)
        self.logger.info(f"å¼€å§‹å½’æ¡£ä½œè€…: {author_name}")
        self.logger.info(f"ä½œè€… URL: {author_url}")
        self.logger.info(f"=" * 60)

        try:
            # å¯åŠ¨æµè§ˆå™¨
            await self.extractor.start()

            # é˜¶æ®µä¸€ï¼šæ”¶é›†æ‰€æœ‰å¸–å­ URLï¼ˆå¸¦ä½œè€…è¿‡æ»¤ï¼‰
            self.logger.info("ã€é˜¶æ®µ 1ã€‘æ”¶é›†å¸–å­ URL...")
            post_urls = await self.extractor.collect_post_urls(
                author_url,
                max_pages,
                author_name=author_name
            )

            # ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶å¸–å­æ•°é‡ï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œï¼‰
            # post_urls = post_urls[:3]  # åªå¤„ç†å‰ 3 ç¯‡å¸–å­

            total_posts = len(post_urls)

            if total_posts == 0:
                self.logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•å¸–å­")
                return {
                    'total': 0,
                    'new': 0,
                    'skipped': 0,
                    'failed': 0
                }

            # é˜¶æ®µäºŒï¼šé€ä¸ªå¤„ç†å¸–å­
            self.logger.info(f"ã€é˜¶æ®µ 2ã€‘å¤„ç† {total_posts} ç¯‡å¸–å­...")
            new_posts = 0
            skipped_posts = 0
            failed_posts = 0

            for idx, post_url in enumerate(post_urls, 1):
                self.logger.info(f"\n--- å¸–å­ {idx}/{total_posts} ---")

                try:
                    # æå–å¸–å­è¯¦æƒ…
                    post_data = await self.extractor.extract_post_details(post_url)

                    if not post_data:
                        self.logger.error(f"æå–å¤±è´¥ï¼Œè·³è¿‡å¸–å­: {post_url}")
                        failed_posts += 1
                        continue

                    # éªŒè¯ä½œè€…åæ˜¯å¦åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼ï¼‰
                    actual_author = post_data['author'].strip()
                    expected_author = author_name.strip()
                    if actual_author.lower() != expected_author.lower():
                        self.logger.warning(
                            f"âš  ä½œè€…ä¸åŒ¹é…ï¼Œè·³è¿‡: {post_data['title']} "
                            f"(å®é™…ä½œè€…: {actual_author}, æœŸæœ›: {expected_author})"
                        )
                        skipped_posts += 1
                        continue

                    # è®¡ç®—ç›®å½•è·¯å¾„
                    post_dir = self._get_post_directory(author_name, post_data)

                    # å¢é‡æ£€æŸ¥
                    if not should_archive(post_dir, post_url):
                        self.logger.info(f"âœ“ è·³è¿‡å·²å½’æ¡£: {post_data['title']}")
                        skipped_posts += 1
                        continue

                    # å½’æ¡£å¸–å­
                    success = await self._archive_post(post_dir, post_data)

                    if success:
                        new_posts += 1
                        self.logger.info(f"âœ“ å½’æ¡£æˆåŠŸ: {post_data['title']}")
                    else:
                        failed_posts += 1
                        self.logger.error(f"âœ— å½’æ¡£å¤±è´¥: {post_data['title']}")

                    # é˜²åçˆ¬å»¶è¿Ÿ
                    if idx < total_posts:
                        await asyncio.sleep(self.rate_limit_delay)

                except Exception as e:
                    self.logger.error(f"å¤„ç†å¸–å­å¤±è´¥: {str(e)}")
                    failed_posts += 1
                    continue

            # æ±‡æ€»ç»Ÿè®¡
            self.logger.info(f"\n" + "=" * 60)
            self.logger.info(f"å½’æ¡£å®Œæˆ: {author_name}")
            self.logger.info(f"  æ€»è®¡: {total_posts} ç¯‡")
            self.logger.info(f"  æ–°å¢: {new_posts} ç¯‡")
            self.logger.info(f"  è·³è¿‡: {skipped_posts} ç¯‡")
            self.logger.info(f"  å¤±è´¥: {failed_posts} ç¯‡")
            self.logger.info(f"=" * 60)

            return {
                'total': total_posts,
                'new': new_posts,
                'skipped': skipped_posts,
                'failed': failed_posts
            }

        except Exception as e:
            self.logger.error(f"å½’æ¡£å¤±è´¥: {str(e)}", exc_info=True)
            raise

        finally:
            await self.extractor.close()

    async def _archive_post(self, post_dir: Path, post_data: Dict) -> bool:
        """å½’æ¡£å•ä¸ªå¸–å­ï¼ˆå¸¦æ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            post_dir: Post directory path
            post_data: Post data dictionary

        Returns:
            True if successful, False otherwise
        """
        try:
            # åˆ›å»ºç›®å½•
            post_dir.mkdir(parents=True, exist_ok=True)

            # è·å–å½’æ¡£è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            progress = get_archive_progress(post_dir)

            # Step 1: ä¿å­˜æ­£æ–‡ï¼ˆå¦‚æœæœªå®Œæˆï¼‰
            if not progress.get('content', False):
                self.logger.info("  â†’ ä¿å­˜æ­£æ–‡...")
                content_file = post_dir / 'content.html'

                # Save metadata + content
                metadata = f"""<!-- å¸–å­å…ƒæ•°æ®
æ ‡é¢˜: {post_data['title']}
ä½œè€…: {post_data['author']}
æ—¶é—´: {post_data['time']}
URL: {post_data['url']}
-->

"""
                full_content = metadata + post_data['content']
                content_file.write_text(full_content, encoding='utf-8')

                progress['content'] = True
                save_archive_progress(post_dir, progress)
                self.logger.info("  âœ“ æ­£æ–‡å·²ä¿å­˜")

            # Step 2: ä¸‹è½½å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ä¸”æœªå®Œæˆï¼‰
            if (self.download_images and
                post_data['images'] and
                not progress.get('images_done', False)):

                self.logger.info(f"  â†’ ä¸‹è½½å›¾ç‰‡ ({len(post_data['images'])} å¼ )...")
                photo_dir = post_dir / 'photo'
                results = await self.downloader.download_files(
                    post_data['images'],
                    photo_dir,
                    prefix='img_'
                )

                progress['images_done'] = True
                save_archive_progress(post_dir, progress)

                success_count = sum(1 for r in results if r)
                self.logger.info(
                    f"  âœ“ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {success_count}/{len(post_data['images'])}"
                )

            # Step 3: ä¸‹è½½è§†é¢‘ï¼ˆå¦‚æœå¯ç”¨ä¸”æœªå®Œæˆï¼‰
            if (self.download_videos and
                post_data['videos'] and
                not progress.get('videos_done', False)):

                self.logger.info(f"  â†’ ä¸‹è½½è§†é¢‘ ({len(post_data['videos'])} ä¸ª)...")
                video_dir = post_dir / 'video'
                results = await self.downloader.download_files(
                    post_data['videos'],
                    video_dir,
                    prefix='video_'
                )

                progress['videos_done'] = True
                save_archive_progress(post_dir, progress)

                success_count = sum(1 for r in results if r)
                self.logger.info(
                    f"  âœ“ è§†é¢‘ä¸‹è½½å®Œæˆ: {success_count}/{len(post_data['videos'])}"
                )

            # æ‰€æœ‰æ­¥éª¤å®Œæˆï¼Œæ ‡è®°å®Œæˆå¹¶åˆ é™¤è¿›åº¦æ–‡ä»¶
            mark_complete(post_dir, post_data['url'])

            progress_file = post_dir / '.progress'
            if progress_file.exists():
                progress_file.unlink()

            return True

        except Exception as e:
            self.logger.error(f"å½’æ¡£å¸–å­å¤±è´¥: {str(e)}", exc_info=True)
            return False

    def _get_post_directory(self, author_name: str, post_data: Dict) -> Path:
        """è®¡ç®—å¸–å­ç›®å½•è·¯å¾„

        Args:
            author_name: Author name
            post_data: Post data dictionary

        Returns:
            Post directory path following structure: author/year/month/YYYY-MM-DD_title
        """
        # è§£æå‘å¸ƒæ—¶é—´
        pub_time = self._parse_time(post_data['time'])

        year = str(pub_time.year)
        month = f"{pub_time.month:02d}"

        # æ ¼å¼åŒ–æ—¥æœŸï¼šYYYY-MM-DD
        date_prefix = pub_time.strftime('%Y-%m-%d')

        # å®‰å…¨åŒ–æ ‡é¢˜
        max_length = self.config.get('storage', {}).get('organization', {}).get(
            'filename_max_length', 100
        )

        # è®¡ç®—æ ‡é¢˜æœ€å¤§é•¿åº¦ï¼šæ€»é•¿åº¦ - æ—¥æœŸé•¿åº¦ - ä¸‹åˆ’çº¿
        # æ ¼å¼ï¼šYYYY-MM-DD_æ ‡é¢˜
        # æ—¥æœŸï¼š10å­—ç¬¦ï¼Œä¸‹åˆ’çº¿ï¼š1å­—ç¬¦
        title_max_length = max_length - 11  # 100 - 11 = 89
        safe_title = sanitize_filename(post_data['title'], max_length=title_max_length)

        # æ„å»ºå¸¦æ—¥æœŸçš„ç›®å½•å
        dir_name = f"{date_prefix}_{safe_title}"

        # æ„å»ºå®Œæ•´è·¯å¾„
        post_dir = self.archive_dir / author_name / year / month / dir_name

        return post_dir

    def _parse_time(self, time_text: str) -> datetime:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²

        Args:
            time_text: Time string from post

        Returns:
            Datetime object
        """
        # Try common formats
        formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d %H:%M',
            '%Y-%m-%d',
            '%Y/%m/%d %H:%M:%S',
            '%Y/%m/%d %H:%M',
            '%Y/%m/%d',
        ]

        for fmt in formats:
            try:
                return datetime.strptime(time_text.strip(), fmt)
            except ValueError:
                continue

        # If parsing fails, use current time
        self.logger.warning(f"æ— æ³•è§£ææ—¶é—´: {time_text}, ä½¿ç”¨å½“å‰æ—¶é—´")
        return datetime.now()
