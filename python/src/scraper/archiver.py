"""Forum archiver - orchestrates extraction and downloading

Two-phase archive process:
1. Collect all post URLs from author pages
2. For each post: extract details, download media, save content

Features:
- Incremental archiving (skip completed posts)
- Resume capability (post-level and file-level)
- Progress tracking with .progress and .complete markers
- Rate limiting to avoid anti-scraping measures
"""

import asyncio
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List
import re
import sys

from jinja2 import Environment, FileSystemLoader

from .extractor import PostExtractor
from .downloader import MediaDownloader
from .utils import (
    sanitize_filename,
    should_archive,
    mark_complete,
    get_archive_progress,
    save_archive_progress
)
from ..utils.logger import setup_logger

# Add parent to path for templates import
sys.path.insert(0, str(Path(__file__).parent.parent))
from templates.filters import clean_html_content, format_file_size


class ForumArchiver:
    """è®ºå›å½’æ¡£å™¨ï¼ˆåè°ƒ Extractor + Downloaderï¼‰"""

    def __init__(self, config: dict):
        """Initialize archiver

        Args:
            config: Configuration dictionary from config.yaml
        """
        self.config = config

        # Extract forum URL (base URL)
        section_url = config['forum']['section_url']
        # Parse base URL from section URL (e.g., https://t66y.com/...)
        match = re.match(r'(https?://[^/]+)', section_url)
        self.base_url = match.group(1) if match else section_url

        self.archive_dir = Path(config['storage']['archive_path'])

        # Setup logging
        project_root = Path(__file__).parent.parent.parent.parent
        log_dir = project_root / 'logs'
        log_dir.mkdir(exist_ok=True)

        self.logger = setup_logger('archiver', log_dir)

        # Initialize sub-components
        self.extractor = PostExtractor(self.base_url, log_dir, config)
        self.downloader = MediaDownloader(
            max_concurrent=config.get('advanced', {}).get('max_concurrent', 5),
            retry_count=config.get('advanced', {}).get('download_retry', 3),
            timeout=config.get('advanced', {}).get('download_timeout', 30),
            log_dir=log_dir
        )

        # Rate limiting delay
        self.rate_limit_delay = config.get('advanced', {}).get('rate_limit_delay', 0.5)

        # Download settings
        self.download_images = config.get('storage', {}).get('download', {}).get('images', True)
        self.download_videos = config.get('storage', {}).get('download', {}).get('videos', True)

        # Initialize Jinja2 template engine
        template_dir = Path(__file__).parent.parent / 'templates'
        self.jinja_env = Environment(loader=FileSystemLoader(str(template_dir)))

        # Register custom filters
        self.jinja_env.filters['clean'] = clean_html_content
        self.jinja_env.filters['size'] = format_file_size

        self.logger.info("æ¨¡æ¿å¼•æ“å·²åˆå§‹åŒ–")

    async def archive_author(
        self,
        author_name: str,
        author_url: str,
        max_pages: Optional[int] = None,
        max_posts: Optional[int] = None
    ) -> Dict:
        """å½’æ¡£ä½œè€…çš„æ‰€æœ‰å¸–å­

        Args:
            author_name: Author name
            author_url: Author's post list URL
            max_pages: Maximum pages to scrape (None = all)
            max_posts: Maximum posts to archive (None = all)

        Returns:
            Statistics dict with keys: total, new, skipped, failed
        """
        self.logger.info(f"=" * 60)
        self.logger.info(f"å¼€å§‹å½’æ¡£ä½œè€…: {author_name}")
        self.logger.info(f"ä½œè€… URL: {author_url}")
        if max_posts:
            self.logger.info(f"é™åˆ¶: æœ€å¤š {max_posts} ç¯‡å¸–å­")
        elif max_pages:
            self.logger.info(f"é™åˆ¶: æœ€å¤š {max_pages} é¡µ")
        self.logger.info(f"=" * 60)

        try:
            # å¯åŠ¨æµè§ˆå™¨
            await self.extractor.start()

            # é˜¶æ®µä¸€ï¼šæ”¶é›†æ‰€æœ‰å¸–å­ URLï¼ˆå¸¦ä½œè€…è¿‡æ»¤ï¼‰
            self.logger.info("ã€é˜¶æ®µ 1ã€‘æ”¶é›†å¸–å­ URL...")
            post_urls = await self.extractor.collect_post_urls(
                author_url,
                max_pages,
                max_posts,
                author_name=author_name
            )

            # ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶å¸–å­æ•°é‡ï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œï¼‰
            # post_urls = post_urls[:3]  # åªå¤„ç†å‰ 3 ç¯‡å¸–å­

            total_posts = len(post_urls)

            # è®ºå›æ€»æ•° = å®é™…æ”¶é›†åˆ°çš„ä¸»é¢˜å¸–æ•°é‡
            # è¯´æ˜ï¼šåªç»Ÿè®¡ä½œè€…ä½œä¸ºæ¥¼ä¸»çš„åŸåˆ›ä¸»é¢˜å¸–ï¼Œä¸åŒ…å«å›å¤åˆ«äººçš„å¸–å­
            forum_total = total_posts
            self.logger.info(
                f"ä½œè€… {author_name} çš„ä¸»é¢˜å¸–æ€»æ•°: {forum_total} "
                f"(åªç»Ÿè®¡æ¥¼ä¸»åŸåˆ›å¸–ï¼Œä¸å«å›å¤)"
            )

            if total_posts == 0:
                self.logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•å¸–å­")
                return {
                    'total': 0,
                    'new': 0,
                    'skipped': 0,
                    'failed': 0
                }

            # é˜¶æ®µäºŒï¼šé€ä¸ªå¤„ç†å¸–å­
            self.logger.info(f"ã€é˜¶æ®µ 2ã€‘å¤„ç† {total_posts} ç¯‡å¸–å­...")
            new_posts = 0
            skipped_posts = 0
            failed_posts = 0

            for idx, post_url in enumerate(post_urls, 1):
                self.logger.info(f"\n--- å¸–å­ {idx}/{total_posts} ---")

                try:
                    # æå–å¸–å­è¯¦æƒ…
                    post_data = await self.extractor.extract_post_details(post_url)

                    if not post_data:
                        self.logger.error(f"æå–å¤±è´¥ï¼Œè·³è¿‡å¸–å­: {post_url}")
                        failed_posts += 1
                        continue

                    # éªŒè¯ä½œè€…åæ˜¯å¦åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼ï¼‰
                    actual_author = post_data['author'].strip()
                    expected_author = author_name.strip()
                    if actual_author.lower() != expected_author.lower():
                        self.logger.warning(
                            f"âš  ä½œè€…ä¸åŒ¹é…ï¼Œè·³è¿‡: {post_data['title']} "
                            f"(å®é™…ä½œè€…: {actual_author}, æœŸæœ›: {expected_author})"
                        )
                        skipped_posts += 1
                        continue

                    # è®¡ç®—ç›®å½•è·¯å¾„
                    post_dir = self._get_post_directory(author_name, post_data)

                    # å¢é‡æ£€æŸ¥
                    if not should_archive(post_dir, post_url):
                        self.logger.info(f"âœ“ è·³è¿‡å·²å½’æ¡£: {post_data['title']}")
                        skipped_posts += 1
                        continue

                    # å½’æ¡£å¸–å­
                    success = await self._archive_post(post_dir, post_data)

                    if success:
                        new_posts += 1
                        self.logger.info(f"âœ“ å½’æ¡£æˆåŠŸ: {post_data['title']}")
                    else:
                        failed_posts += 1
                        self.logger.error(f"âœ— å½’æ¡£å¤±è´¥: {post_data['title']}")

                    # é˜²åçˆ¬å»¶è¿Ÿ
                    if idx < total_posts:
                        await asyncio.sleep(self.rate_limit_delay)

                except Exception as e:
                    self.logger.error(f"å¤„ç†å¸–å­å¤±è´¥: {str(e)}")
                    failed_posts += 1
                    continue

            # æ±‡æ€»ç»Ÿè®¡
            self.logger.info(f"\n" + "=" * 60)
            self.logger.info(f"å½’æ¡£å®Œæˆ: {author_name}")
            self.logger.info(f"  æ€»è®¡: {total_posts} ç¯‡")
            self.logger.info(f"  æ–°å¢: {new_posts} ç¯‡")
            self.logger.info(f"  è·³è¿‡: {skipped_posts} ç¯‡")
            self.logger.info(f"  å¤±è´¥: {failed_posts} ç¯‡")
            self.logger.info(f"=" * 60)

            return {
                'total': total_posts,
                'new': new_posts,
                'skipped': skipped_posts,
                'failed': failed_posts,
                'forum_total': forum_total  # æ–°å¢ï¼šè¿”å›è®ºå›æ€»æ•°
            }

        except Exception as e:
            self.logger.error(f"å½’æ¡£å¤±è´¥: {str(e)}", exc_info=True)
            raise

        finally:
            await self.extractor.close()

    async def _archive_post(self, post_dir: Path, post_data: Dict) -> bool:
        """å½’æ¡£å•ä¸ªå¸–å­ï¼ˆå¸¦æ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            post_dir: Post directory path
            post_data: Post data dictionary

        Returns:
            True if successful, False otherwise
        """
        try:
            # åˆ›å»ºç›®å½•
            post_dir.mkdir(parents=True, exist_ok=True)

            # è·å–å½’æ¡£è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            progress = get_archive_progress(post_dir)

            # Step 1: ä¸‹è½½å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ä¸”æœªå®Œæˆï¼‰
            if (self.download_images and
                post_data['images'] and
                not progress.get('images_done', False)):

                self.logger.info(f"  â†’ ä¸‹è½½å›¾ç‰‡ ({len(post_data['images'])} å¼ )...")
                photo_dir = post_dir / 'photo'
                results = await self.downloader.download_files(
                    post_data['images'],
                    photo_dir,
                    prefix='img_'
                )

                progress['images_done'] = True
                save_archive_progress(post_dir, progress)

                success_count = sum(1 for r in results if r)
                self.logger.info(
                    f"  âœ“ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {success_count}/{len(post_data['images'])}"
                )

            # Step 2: ä¸‹è½½è§†é¢‘ï¼ˆå¦‚æœå¯ç”¨ä¸”æœªå®Œæˆï¼‰
            if (self.download_videos and
                post_data['videos'] and
                not progress.get('videos_done', False)):

                self.logger.info(f"  â†’ ä¸‹è½½è§†é¢‘ ({len(post_data['videos'])} ä¸ª)...")
                video_dir = post_dir / 'video'
                results = await self.downloader.download_files(
                    post_data['videos'],
                    video_dir,
                    prefix='video_'
                )

                progress['videos_done'] = True
                save_archive_progress(post_dir, progress)

                success_count = sum(1 for r in results if r)
                self.logger.info(
                    f"  âœ“ è§†é¢‘ä¸‹è½½å®Œæˆ: {success_count}/{len(post_data['videos'])}"
                )

            # Step 3: ç”Ÿæˆ content.htmlï¼ˆä½¿ç”¨æ–°æ¨¡æ¿ï¼‰
            # æ³¨æ„ï¼šåœ¨åª’ä½“ä¸‹è½½å®Œæˆåç”Ÿæˆï¼Œè¿™æ ·å¯ä»¥æ­£ç¡®åˆ—å‡ºæœ¬åœ°æ–‡ä»¶
            if not progress.get('content', False):
                self.logger.info("  â†’ ç”Ÿæˆ content.html...")
                self._save_content_html(post_data, post_dir)

                progress['content'] = True
                save_archive_progress(post_dir, progress)

            # æ‰€æœ‰æ­¥éª¤å®Œæˆï¼Œæ ‡è®°å®Œæˆå¹¶åˆ é™¤è¿›åº¦æ–‡ä»¶
            mark_complete(post_dir, post_data['url'])

            progress_file = post_dir / '.progress'
            if progress_file.exists():
                progress_file.unlink()

            return True

        except Exception as e:
            self.logger.error(f"å½’æ¡£å¸–å­å¤±è´¥: {str(e)}", exc_info=True)
            return False

    def _prepare_media_list(self, media_urls: List[str], media_type: str, post_dir: Path) -> List[Dict]:
        """å‡†å¤‡åª’ä½“æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºæ¨¡æ¿ï¼‰

        Args:
            media_urls: åŸå§‹ URL åˆ—è¡¨
            media_type: 'image' æˆ– 'video'
            post_dir: å¸–å­ç›®å½•

        Returns:
            [{'filename': 'img_1.jpg', 'url': '...', 'size': '1.2 MB'}, ...]
        """
        media_list = []

        # ç¡®å®šå­ç›®å½•å’Œæ–‡ä»¶å‰ç¼€
        if media_type == 'image':
            subdir = post_dir / 'photo'
            prefix = 'img_'
            extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        else:  # video
            subdir = post_dir / 'video'
            prefix = 'video_'
            extensions = ['.mp4', '.avi', '.mkv', '.webm', '.mov']

        if not subdir.exists():
            return []

        # éå†åŸå§‹ URLï¼ŒåŒ¹é…æœ¬åœ°æ–‡ä»¶
        for idx, url in enumerate(media_urls, 1):
            # å°è¯•æ‰¾åˆ°å¯¹åº”çš„æœ¬åœ°æ–‡ä»¶
            filename = None
            file_size = None

            # æ–¹æ³•1ï¼šæŒ‰ç´¢å¼•åŒ¹é…ï¼ˆimg_1.jpg, img_2.jpg...ï¼‰
            for ext in extensions:
                candidate = subdir / f"{prefix}{idx}{ext}"
                if candidate.exists():
                    filename = f"{prefix}{idx}{ext}"
                    file_size = candidate.stat().st_size
                    break

            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨å ä½
            if not filename:
                filename = f"{prefix}{idx}.unknown"

            media_list.append({
                'filename': filename,
                'url': url,
                'size': format_file_size(file_size) if file_size else None
            })

        return media_list

    def _save_content_html(self, post_data: Dict, post_dir: Path):
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå¹¶ä¿å­˜ content.html

        Args:
            post_data: å¸–å­æ•°æ®ï¼ˆåŒ…å« title, author, time, content, images, videos, urlï¼‰
            post_dir: å¸–å­ç›®å½•
        """
        try:
            # å‡†å¤‡æ¨¡æ¿æ•°æ®
            template_data = {
                'title': post_data.get('title', 'æ— æ ‡é¢˜'),
                'author': post_data.get('author', 'æœªçŸ¥ä½œè€…'),
                'publish_time': post_data.get('time', 'N/A'),
                'archive_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'url': post_data.get('url', ''),
                'content': clean_html_content(post_data.get('content', '')),
                'content_length': len(post_data.get('content', '')),
                'images': self._prepare_media_list(
                    post_data.get('images', []),
                    'image',
                    post_dir
                ),
                'videos': self._prepare_media_list(
                    post_data.get('videos', []),
                    'video',
                    post_dir
                )
            }

            # åŠ è½½æ¨¡æ¿
            template = self.jinja_env.get_template('post.html')

            # æ¸²æŸ“ HTML
            html = template.render(**template_data)

            # ä¿å­˜æ–‡ä»¶
            content_file = post_dir / 'content.html'
            content_file.write_text(html, encoding='utf-8')

            self.logger.info(f"  âœ“ å·²ç”Ÿæˆ content.html")

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆ content.html å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def _get_post_directory(self, author_name: str, post_data: Dict) -> Path:
        """è®¡ç®—å¸–å­ç›®å½•è·¯å¾„

        Args:
            author_name: Author name
            post_data: Post data dictionary

        Returns:
            Post directory path following structure: author/year/month/YYYY-MM-DD_title
        """
        # è§£æå‘å¸ƒæ—¶é—´
        pub_time = self._parse_time(post_data['time'])

        year = str(pub_time.year)
        month = f"{pub_time.month:02d}"

        # æ ¼å¼åŒ–æ—¥æœŸï¼šYYYY-MM-DD
        date_prefix = pub_time.strftime('%Y-%m-%d')

        # å®‰å…¨åŒ–æ ‡é¢˜
        max_length = self.config.get('storage', {}).get('organization', {}).get(
            'filename_max_length', 100
        )

        # è®¡ç®—æ ‡é¢˜æœ€å¤§é•¿åº¦ï¼šæ€»é•¿åº¦ - æ—¥æœŸé•¿åº¦ - ä¸‹åˆ’çº¿
        # æ ¼å¼ï¼šYYYY-MM-DD_æ ‡é¢˜
        # æ—¥æœŸï¼š10å­—ç¬¦ï¼Œä¸‹åˆ’çº¿ï¼š1å­—ç¬¦
        title_max_length = max_length - 11  # 100 - 11 = 89
        safe_title = sanitize_filename(post_data['title'], max_length=title_max_length)

        # æ„å»ºå¸¦æ—¥æœŸçš„ç›®å½•å
        dir_name = f"{date_prefix}_{safe_title}"

        # æ„å»ºå®Œæ•´è·¯å¾„
        post_dir = self.archive_dir / author_name / year / month / dir_name

        return post_dir

    def _parse_time(self, time_text: str) -> datetime:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²

        Args:
            time_text: Time string from post

        Returns:
            Datetime object
        """
        # Try common formats
        formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d %H:%M',
            '%Y-%m-%d',
            '%Y/%m/%d %H:%M:%S',
            '%Y/%m/%d %H:%M',
            '%Y/%m/%d',
        ]

        for fmt in formats:
            try:
                return datetime.strptime(time_text.strip(), fmt)
            except ValueError:
                continue

        # If parsing fails, use current time
        self.logger.warning(f"æ— æ³•è§£ææ—¶é—´: {time_text}, ä½¿ç”¨å½“å‰æ—¶é—´")
        return datetime.now()
